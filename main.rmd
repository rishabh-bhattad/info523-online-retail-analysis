---
title: "INFO 523 – Online Retail Data Mining Project"
author: "Rishabh Bhattad"
output:
  word_document: default
---

# Introduction

This project uses the Kaggle Online Retail dataset to perform association rule mining, clustering, and classification.

# Setup
```{r}
install.packages("factoextra")
install.packages("caret")
install.packages("tidyverse")
install.packages("randomForest")
```

```{r}
library(tidyverse)
library(lubridate)
library(arules)
library(arulesViz)
library(cluster)
library(factoextra)
library(caret)
library(reshape2)
library(pROC)
library(randomForest)

set.seed(42)
```

# Load Data

```{r}
retail_raw <- read_csv("data/online_retail.csv")
glimpse(retail_raw)
```

# Basic Cleaning & EDA

```{r}
summary(retail_raw)
sum(is.na(retail_raw$CustomerID))
```

# Data Cleaning & Feature Engineering

```{r}
retail_clean <- retail_raw %>%
  # remove missing customer IDs
  filter(!is.na(CustomerID)) %>%
  filter(Quantity > 0, UnitPrice > 0) %>%
  # remove cancelled invoices
  filter(!str_starts(InvoiceNo, "C")) %>%
  
  mutate(
    InvoiceDate = lubridate::parse_date_time(InvoiceDate,
                                             orders = c("d/m/Y H:M", "d/m/Y H:M:S",
                                                        "m/d/Y H:M", "m/d/Y H:M:S")),
    InvoiceYear = lubridate::year(InvoiceDate),
    InvoiceMonth = lubridate::floor_date(InvoiceDate, unit = "month"),
    TotalPrice = Quantity * UnitPrice
  )

# check cleaned data
glimpse(retail_clean)
summary(retail_clean$Quantity)
summary(retail_clean$UnitPrice)
sum(is.na(retail_clean$CustomerID))
n_distinct(retail_clean$CustomerID)
```

# Association Rule Mining

```{r}
# filter UK data
retail_uk <- retail_clean %>%
  filter(Country == "United Kingdom")

# build invoice-item pairs
trans_df <- retail_uk %>%
  select(InvoiceNo, Description) %>%
  distinct()

# make transactions
trans_list <- split(trans_df$Description, trans_df$InvoiceNo)
retail_trans <- as(trans_list, "transactions")

# show transaction summary
retail_trans
summary(retail_trans)

# Run Apriori algorithm
rules <- apriori(
  retail_trans,
  parameter = list(supp = 0.001, conf = 0.5, minlen = 2)
)

# top rules
summary(rules)
rules_top <- sort(rules, by = "lift", decreasing = TRUE)
inspect(head(rules_top, 20))

# barplot of top rules
rules_df <- as(rules_top[1:15], "data.frame")

library(ggplot2)

p <- ggplot(rules_df, aes(x = reorder(rules, lift), y = lift)) +
  geom_col(fill = "#2E86C1") +
  coord_flip() +
  labs(
    title = "Top 15 Association Rules by Lift",
    x = "Rule",
    y = "Lift"
  ) +
  theme_minimal(base_size = 12)

p

# save plot
png("plots/assoc_rules_barplot.png", width = 1400, height = 1000)
print(p)
dev.off()
```

# Customer Clustering

```{r}
# build customer features
cust_df <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(
    total_spent = sum(TotalPrice),
    total_qty = sum(Quantity),
    num_orders = n_distinct(InvoiceNo),
    avg_order = mean(TotalPrice)
  )

# remove extreme spenders (top 1%) for more stable clusters
q99 <- quantile(cust_df$total_spent, 0.99, na.rm = TRUE)
cust_df <- cust_df %>%
  filter(total_spent <= q99)

# log transform to reduce skew
cust_feats <- cust_df %>%
  mutate(
    log_total_spent = log1p(total_spent),
    log_total_qty = log1p(total_qty),
    log_num_orders = log1p(num_orders),
    log_avg_order = log1p(avg_order)
  ) %>%
  select(CustomerID, starts_with("log_"))

# scale data
cust_scaled <- scale(cust_feats[, -1])

# find 3 clusters
set.seed(42)
km <- kmeans(cust_scaled, centers = 3, nstart = 25)

# add cluster labels
cust_feats$cluster <- km$cluster

# assign readable segment names
cust_feats$segment <- dplyr::case_match(
  cust_feats$cluster,
  1 ~ "High-Value Frequent Buyers",
  2 ~ "Low-Value Casual Buyers",
  3 ~ "Large-Basket Buyers"
)

# view segment sizes
table(cust_feats$segment)

# barplot of cluster sizes
cluster_sizes <- as.data.frame(table(cust_feats$segment))
colnames(cluster_sizes) <- c("segment", "count")

p1 <- ggplot(cluster_sizes, aes(x = segment, y = count, fill = segment)) +
  geom_col() +
  labs(
    title = "Customer Segments (Defined by Purchasing Behavior)",
    x = "Segment",
    y = "Customer count",
    fill = "Segment"
  ) +
  theme_minimal(base_size = 12)

p1

# save barplot
png("plots/cluster_sizes.png", width = 1200, height = 900)
print(p1)
dev.off()

# join cluster labels back to original (unlogged) features
cust_summary <- cust_df %>%
  inner_join(cust_feats %>% select(CustomerID, segment), by = "CustomerID") %>%
  group_by(segment) %>%
  summarise(
    avg_total_spent = mean(total_spent),
    avg_total_qty = mean(total_qty),
    avg_num_orders = mean(num_orders),
    avg_order_value = mean(avg_order),
    .groups = "drop"
  )

# reshape for profile plot
cluster_long <- cust_summary %>%
  tidyr::pivot_longer(
    cols = c(avg_total_spent, avg_total_qty, avg_num_orders, avg_order_value),
    names_to = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = dplyr::recode(
      metric,
      "avg_total_spent" = "Total spent (GBP)",
      "avg_total_qty" = "Total quantity",
      "avg_num_orders" = "Number of orders",
      "avg_order_value" = "Avg order value (GBP)"
    )
  )

p2 <- ggplot(cluster_long, aes(x = segment, y = value, fill = segment)) +
  geom_col(position = "dodge") +
  facet_wrap(~ metric, scales = "free_y") +
  labs(
    title = "Customer Segment Profiles",
    x = "Segment",
    y = "Average metric value",
    fill = "Segment"
  ) +
  theme_minimal(base_size = 12)

p2

# save profile plot
png("plots/cluster_segment_profile.png", width = 1600, height = 900)
print(p2)
dev.off()
```


# Classification

```{r}
# create label for high-value customers
cust_summary2 <- cust_df %>%
  inner_join(cust_feats %>% select(CustomerID, segment), by = "CustomerID") %>%
  mutate(label = ifelse(segment == "High-Value Frequent Buyers", "High", "Not_High")) %>%
  mutate(label = factor(label))

# split data
set.seed(42)
train_index <- createDataPartition(cust_summary2$label, p = 0.8, list = FALSE)
train_data <- cust_summary2[train_index, ]
test_data <- cust_summary2[-train_index, ]

# build features
train_x <- train_data %>% select(total_spent, total_qty, num_orders, avg_order)
train_y <- train_data$label

test_x <- test_data %>% select(total_spent, total_qty, num_orders, avg_order)
test_y <- test_data$label

# train decision tree
model_dt <- train(
  train_x, train_y,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 5)
)

model_dt

# predictions
pred_dt <- predict(model_dt, test_x)

# confusion matrix
cm_dt <- confusionMatrix(pred_dt, test_y)
cm_dt

# plot variable importance
imp_dt <- varImp(model_dt)$importance
imp_dt$Feature <- rownames(imp_dt)

p3 <- ggplot(imp_dt, aes(x = reorder(Feature, Overall), y = Overall, fill = Feature)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Feature Importance for Predicting High-Value Customers",
    subtitle = "Higher score indicates stronger influence in classification decision",
    x = "Customer Features",
    y = "Importance Score"
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

p3

# save importance plot
png("plots/classification_feature_importance.png", width = 1400, height = 1000)
print(p3)
dev.off()
```

# ROC Curve (Decision Tree vs Random Forest)

```{r}
# probability predictions for ROC
dt_prob <- predict(model_dt, test_x, type = "prob")[, "High"]

# train random forest
set.seed(42)
model_rf <- randomForest(
  label ~ total_spent + total_qty + num_orders + avg_order,
  data = train_data,
  ntree = 300
)

# RF probabilities
rf_prob <- predict(model_rf, test_x, type = "prob")[, "High"]

# ROC objects
roc_dt <- roc(test_y, dt_prob)
roc_rf <- roc(test_y, rf_prob)

# plot ROC curves
png("plots/classification_roc_curve.png", width = 1400, height = 1000)

plot(roc_dt, col = "#2E86C1", lwd = 3,
     main = "ROC Curve: Decision Tree vs Random Forest",
     legacy.axes = TRUE)
lines(roc_rf, col = "#28B463", lwd = 3)

auc_dt <- auc(roc_dt)
auc_rf <- auc(roc_rf)

text(0.65, 0.25, paste0("Decision Tree AUC = ", round(auc_dt, 3)),
     col = "#2E86C1", cex = 1.4)
text(0.65, 0.15, paste0("Random Forest AUC = ", round(auc_rf, 3)),
     col = "#28B463", cex = 1.4)

better_model <- ifelse(auc_rf > auc_dt,
                       "Random Forest performs better",
                       "Decision Tree performs better")

text(0.65, 0.05, better_model, col = "black", font = 2, cex = 1.4)

legend("bottomright",
       legend = c("Decision Tree", "Random Forest"),
       col = c("#2E86C1", "#28B463"), lwd = 3)

dev.off()
```

# Random Forest Feature Importance

```{r}
rf_imp <- importance(model_rf)
rf_imp_df <- data.frame(
  Feature = rownames(rf_imp),
  Importance = rf_imp[, 1]
)

p4 <- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Feature)) +
  geom_col(show.legend = FALSE, width = 0.7) +
  coord_flip() +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Random Forest Feature Importance",
    subtitle = "Higher bars mean a stronger contribution to predicting High-Value customers",
    x = "Customer Features",
    y = "Importance Score"
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

p4

png("plots/classification_rf_importance.png", width = 1400, height = 1000)
print(p4)
dev.off()
```

# Classification Performance Comparison

```{r}
# dt confusion matrix
cm_dt <- confusionMatrix(pred_dt, test_y)

# rf predictions + confusion matrix
pred_rf <- predict(model_rf, test_x)
cm_rf <- confusionMatrix(pred_rf, test_y)

cm_dt
cm_rf
```

```{r}
performance_table <- data.frame(
  Model = c("Decision Tree", "Random Forest"),
  Accuracy = c(cm_dt$overall["Accuracy"], cm_rf$overall["Accuracy"]),
  Sensitivity = c(cm_dt$byClass["Sensitivity"], cm_rf$byClass["Sensitivity"]),
  Specificity = c(cm_dt$byClass["Specificity"], cm_rf$byClass["Specificity"]),
  AUC = c(as.numeric(auc(roc_dt)), as.numeric(auc(roc_rf)))
)

knitr::kable(performance_table, digits = 3,
             caption = "Classification Performance: Decision Tree vs Random Forest")
```

# Final Conclusion

1. Association rules revealed meaningful product pairings based on lift.
2. Clustering created three clear customer segments:
   • High-Value Frequent Buyers  
   • Low-Value Casual Buyers  
   • Large-Basket Buyers  

3. Classification identified key drivers of high-value customers:
   • Decision Tree: total_spent, num_orders strongest predictors  
   • Random Forest achieved higher AUC and accuracy

4. ROC analysis shows Random Forest outperforms Decision Tree consistently.

Overall, the data mining tasks—association mining, clustering, and classification—
provide a complete view of customer behavior and help identify high-value targets.
